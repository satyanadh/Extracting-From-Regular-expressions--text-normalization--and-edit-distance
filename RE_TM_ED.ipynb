{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regular expressions, text normalization, and edit distance"
      ],
      "metadata": {
        "id": "eGajpTvZbl8W"
      },
      "id": "eGajpTvZbl8W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Initialization & Setup"
      ],
      "metadata": {
        "id": "lkzAo9kSbpaa"
      },
      "id": "lkzAo9kSbpaa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56c92a0",
      "metadata": {
        "id": "d56c92a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe79a32b-e47f-4c2d-dde8-df1be7b5d5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# importing required libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Regular Expressions"
      ],
      "metadata": {
        "id": "2jEV5pB3bhkq"
      },
      "id": "2jEV5pB3bhkq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting license plate numbers, IDs, emails and mailing addresses from a document\n"
      ],
      "metadata": {
        "id": "oLP8HLkcbvcG"
      },
      "id": "oLP8HLkcbvcG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document creation"
      ],
      "metadata": {
        "id": "xLT-yj6keZOF"
      },
      "id": "xLT-yj6keZOF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa7f39",
      "metadata": {
        "id": "aefa7f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e4a118a9-09a9-46d5-8c05-dad8bfc36b14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "sentence = 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'\n",
        "sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting license plate numbers"
      ],
      "metadata": {
        "id": "enF7P05qebea"
      },
      "id": "enF7P05qebea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e2f32e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e2f32e",
        "outputId": "307047ff-f5fd-4008-d50c-14dcc0f08918"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4XUI302', '3A-278']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# The format of license plate number is a digit then 2 or 3 letters (one of which can be a \"-\"), and then 3 digits\n",
        "\n",
        "regex = re.compile(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})')\n",
        "lincense_plate_numbers = regex.findall(sentence)\n",
        "lincense_plate_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-1: Extract the ID numbers from the document."
      ],
      "metadata": {
        "id": "nzg5Gxx9dzW2"
      },
      "id": "nzg5Gxx9dzW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98e769f",
      "metadata": {
        "id": "d98e769f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ab729b-2acb-4ce3-dc1d-7ff7ad12c72a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J987492']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# The format of the IDs is one character/letter and then 6 digits\n",
        "regex = re.compile(r'\\b[A-Za-z]\\d{6}\\b')\n",
        "ids = regex.findall(sentence)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-2: Extract the email IDs from the document"
      ],
      "metadata": {
        "id": "y3BZc47FeRzR"
      },
      "id": "y3BZc47FeRzR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11287af4",
      "metadata": {
        "id": "11287af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d687879-56aa-4f68-effe-939a1625b12a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['myemail123+spam@google.cg', 'jane.doe@sjsu.edu']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "emails = regex.findall(sentence)\n",
        "emails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-3: Extract the mailing address from the document"
      ],
      "metadata": {
        "id": "UfCxo2u2erDf"
      },
      "id": "UfCxo2u2erDf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62515169",
      "metadata": {
        "id": "62515169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6dd310-8d68-4021-ea7a-a17ca37edff4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "regex = re.compile(r'\\b\\d+\\s+[A-Za-z\\s]*,\\s*[A-Za-z\\s]*,\\s*[A-Z]{2}\\.\\b')\n",
        "mailing_address = regex.findall(sentence)\n",
        "mailing_address"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-4: Anonymize the license plate numbers by replacing them with the text \"LP_NUM\"\n",
        "\n",
        "The re.sub function is described here: https://docs.python.org/3/library/re.html"
      ],
      "metadata": {
        "id": "uM53UdvPevrA"
      },
      "id": "uM53UdvPevrA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6ae08c",
      "metadata": {
        "id": "ca6ae08c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a3b8585c-7c30-4b93-a4d4-9ac59bcb4e2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Now replacing license plate numbers with the string \"LP_NUM\"\n",
        "regex = re.compile(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})')\n",
        "sentence_modified = re.sub(regex, 'LP_NUM', sentence)\n",
        "sentence_modified"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-5: Replace the ID numbers with the text \"ID_NUM\""
      ],
      "metadata": {
        "id": "rArCsPyMfAeZ"
      },
      "id": "rArCsPyMfAeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[A-Za-z]\\d{6}\\b')\n",
        "sentence_modified = re.sub(regex, 'LP_NUM', sentence)\n",
        "sentence_modified"
      ],
      "metadata": {
        "id": "glVWmQAOfFTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "867b8df3-3d29-48d8-c0f7-987f0b75bc5b"
      },
      "id": "glVWmQAOfFTU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is LP_NUM and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Text Processing"
      ],
      "metadata": {
        "id": "q2ymEa7sfHnL"
      },
      "id": "q2ymEa7sfHnL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of words in the movie_reviews dataset (dataset uploaded in the beginning of this notebook under \"Part 0: Initialization and Setup\")"
      ],
      "metadata": {
        "id": "F_Y77JT9fjid"
      },
      "id": "F_Y77JT9fjid"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3239e17c",
      "metadata": {
        "id": "3239e17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a974e0cd-a085-47b0-dd82-5afc15475738"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1583820"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# print number of words in the movie review dataset\n",
        "len(movie_reviews.words())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the standard list of punctuation marks"
      ],
      "metadata": {
        "id": "86nbNEYwfgwl"
      },
      "id": "86nbNEYwfgwl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0929c795",
      "metadata": {
        "id": "0929c795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da580f21-a80a-4fd7-b878-8029d21c4aa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "punctuations = string.punctuation\n",
        "punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctation from movie reviews\n"
      ],
      "metadata": {
        "id": "sg5Sc7X9fbpg"
      },
      "id": "sg5Sc7X9fbpg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18760b2a",
      "metadata": {
        "id": "18760b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815d97f2-9a84-4e40-e08d-a7af18f9e62f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1338788"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "words_wo_puncts = [x for x in movie_reviews.words() if x not in punctuations]\n",
        "len(words_wo_puncts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of unique words"
      ],
      "metadata": {
        "id": "Gvl4C9l8f0M-"
      },
      "id": "Gvl4C9l8f0M-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a18f452",
      "metadata": {
        "id": "0a18f452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0345ffa2-a1b8-4053-86b9-123c06bfdf08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39737"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "unique_words = set(words_wo_puncts)\n",
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the 20 most frequent words in the dataset"
      ],
      "metadata": {
        "id": "D2L7yKz3gL-h"
      },
      "id": "D2L7yKz3gL-h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784d9a68",
      "metadata": {
        "id": "784d9a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d37090-5a56-4aad-c1ca-a310164a6119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the     76529\n",
              "a       38106\n",
              "and     35576\n",
              "of      34123\n",
              "to      31937\n",
              "is      25195\n",
              "in      21822\n",
              "s       18513\n",
              "it      16107\n",
              "that    15924\n",
              "as      11378\n",
              "with    10792\n",
              "for      9961\n",
              "his      9587\n",
              "this     9578\n",
              "film     9517\n",
              "i        8889\n",
              "he       8864\n",
              "but      8634\n",
              "on       7385\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# top 20 highest freq words\n",
        "pd.Series(words_wo_puncts).value_counts()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the standard list of stopwords"
      ],
      "metadata": {
        "id": "id9YqucXf6oW"
      },
      "id": "id9YqucXf6oW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de2e57e",
      "metadata": {
        "id": "9de2e57e"
      },
      "outputs": [],
      "source": [
        "# getting english stopwords\n",
        "eng_stopwords = stopwords.words('english')\n",
        "eng_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of stopwords"
      ],
      "metadata": {
        "id": "cL-RrEeCgA0Z"
      },
      "id": "cL-RrEeCgA0Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6645fe0",
      "metadata": {
        "id": "d6645fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f968b152-b768-40b4-e855-83f01f2f6825"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(eng_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-1: Remove the stopwords from the dataset (similarly to how we removed punctuation above)"
      ],
      "metadata": {
        "id": "xBAO2VeWgDiM"
      },
      "id": "xBAO2VeWgDiM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d584a6f",
      "metadata": {
        "id": "9d584a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28943e9-0148-4a80-a808-a3d921d12601"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "955610"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "words_wo_puncts_stopwords = [x for x in movie_reviews.words() if x not in eng_stopwords]\n",
        "len(words_wo_puncts_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-2: Find the number of uniques words in the dataset now that the stop words have been removed"
      ],
      "metadata": {
        "id": "RuUty50kgS2o"
      },
      "id": "RuUty50kgS2o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92f5eb5",
      "metadata": {
        "id": "a92f5eb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396ce11b-8503-4d67-d63a-e47b3eecf22e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39617"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# unique words without stopwords\n",
        "unique_words = set(words_wo_puncts_stopwords)\n",
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-3: Find the top 20 highest frequency words now that we have removed the stopwords"
      ],
      "metadata": {
        "id": "bxDxQWNegcny"
      },
      "id": "bxDxQWNegcny"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840dded3",
      "metadata": {
        "id": "840dded3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "a05ad2b1-64a7-4992-8b30-abb558921828"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d9fb26e394da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# top 20 highest freq words after removing stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords_wo_puncts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_wo_puncts_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_wo_puncts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
          ]
        }
      ],
      "source": [
        "# top 20 highest freq words after removing stopwords\n",
        "\n",
        "words_wo_puncts = [x for x in words_wo_puncts_stopwords.words() if x not in punctuations]\n",
        "len(words_wo_puncts)\n",
        "\n",
        "pd.Series(words_wo_puncts).value_counts()[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the words that are used only once in the corpus (and print the first few).  "
      ],
      "metadata": {
        "id": "9Z36G7BcgmDF"
      },
      "id": "9Z36G7BcgmDF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70748ea0",
      "metadata": {
        "id": "70748ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86d1d5c-07e9-49cd-896e-4894a6364c48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['looooot',\n",
              " 'schnazzy',\n",
              " 'timex',\n",
              " 'indiglo',\n",
              " 'jessalyn',\n",
              " 'gilsig',\n",
              " 'ruber',\n",
              " 'jaleel',\n",
              " 'balki',\n",
              " 'wavers',\n",
              " 'statistics',\n",
              " 'snapshot',\n",
              " 'guesswork',\n",
              " 'maryam',\n",
              " 'daylights',\n",
              " 'terraformed',\n",
              " 'stagnated',\n",
              " 'napolean',\n",
              " 'millimeter',\n",
              " 'enmeshed']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# 20 words that are used only once in corpus using hapaxes() function\n",
        "nltk.FreqDist(words_wo_puncts_stopwords).hapaxes()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-4: Use the PorterStemmer to stem the words in the dataset.\n",
        "\n",
        "Display the first few words."
      ],
      "metadata": {
        "id": "US3mRSQ8bDei"
      },
      "id": "US3mRSQ8bDei"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words = nltk.word_tokenize(movie_reviews.words)\n",
        "\n",
        "# Create a Porter Stemmer instance\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem the words in the dataset\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Display the first few stemmed words\n",
        "print(\"Stemmed words:\", stemmed_words[:5])\n"
      ],
      "metadata": {
        "id": "nX3r9FfubKdB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "ffb864bc-5c71-40c0-c299-42aed2f6a271"
      },
      "id": "nX3r9FfubKdB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-97185e226a31>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a Porter Stemmer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-5: Use the WordNetLemmatizer to lemmatize the words in the dataset.\n",
        "\n",
        "Display the first few words."
      ],
      "metadata": {
        "id": "QEVGhVGTbUMT"
      },
      "id": "QEVGhVGTbUMT"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "WZxMzKv4bMdl"
      },
      "id": "WZxMzKv4bMdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uMhXtA3RbMJk"
      },
      "id": "uMhXtA3RbMJk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-6:\n",
        "a) How many unique words are there once stemming is applied? (show the that performs the computation and outputs the result)\n",
        "\n",
        "b) How many unique words are there once lemmatization is applied? (show the code that performs the computation and outputs the result)"
      ],
      "metadata": {
        "id": "LuWCQWX3bnsD"
      },
      "id": "LuWCQWX3bnsD"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4WAh6UEbqNq"
      },
      "id": "N4WAh6UEbqNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Tokenization"
      ],
      "metadata": {
        "id": "FQOoke3_bvbr"
      },
      "id": "FQOoke3_bvbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3-1: Use the Penn Tree Bank tokenizer to tokenize the sentence below\n",
        "\n",
        "Print the tokens that the tokenizer produces."
      ],
      "metadata": {
        "id": "o0HA5ds8HL6-"
      },
      "id": "o0HA5ds8HL6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204dbae3",
      "metadata": {
        "id": "204dbae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a93b11-d014-4844-d0f9-ca827b2b2b6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Please',\n",
              " 'pay',\n",
              " '$',\n",
              " '100.55',\n",
              " 'to',\n",
              " 'settle',\n",
              " 'your',\n",
              " 'bill.',\n",
              " 'Send',\n",
              " 'confirmation',\n",
              " 'to',\n",
              " 'confirm',\n",
              " '@',\n",
              " 'gmail.com',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = 'Please pay $100.55 to settle your bill.  Send confirmation to confirm@gmail.com.'\n",
        "\n",
        "# Creating a Penn Treebank tokenizer instance\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(s)\n",
        "tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Levenshtein Distance & Alignment\n",
        "\n",
        "Relevant nltk documentation: https://www.nltk.org/api/nltk.metrics.distance.html"
      ],
      "metadata": {
        "id": "Tu9xug2Gxr84"
      },
      "id": "Tu9xug2Gxr84"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4-1: Use the nltk functions edit_distance to compute the Levenshtein edit-distance between the strings \"intention\" and \"execution\""
      ],
      "metadata": {
        "id": "fBsXnDQ-yPPE"
      },
      "id": "fBsXnDQ-yPPE"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "# Define the two strings\n",
        "string1 = \"intention\"\n",
        "string2 = \"execution\"\n",
        "\n",
        "# Compute the Levenshtein edit distance\n",
        "distance = edit_distance(string1, string2)\n",
        "\n",
        "# Print the result\n",
        "print(\"Levenshtein edit distance between '{}' and '{}' is: {}\".format(string1, string2, distance))\n"
      ],
      "metadata": {
        "id": "5aaSK4Ehylz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201486a4-31eb-4576-a81d-486fc63d3a1d"
      },
      "id": "5aaSK4Ehylz7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levenshtein edit distance between 'intention' and 'execution' is: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4-2: Use the nltk function edit_distance_align to compute the minimum Levenshtein edit-distance based alignment mapping between the two strings \"intention\" and \"execution\""
      ],
      "metadata": {
        "id": "NKWLhn1RzBGv"
      },
      "id": "NKWLhn1RzBGv"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance_align\n",
        "\n",
        "# Define the two strings\n",
        "string1 = \"intention\"\n",
        "string2 = \"execution\"\n",
        "\n",
        "# Compute the alignment mapping\n",
        "alignment_mapping = edit_distance_align(string1, string2)\n",
        "\n",
        "alignment_mapping"
      ],
      "metadata": {
        "id": "Zc16veVuzBxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e3e56a-3b99-4001-d9d3-ec2404f84baf"
      },
      "id": "Zc16veVuzBxM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (1, 1),\n",
              " (2, 2),\n",
              " (3, 3),\n",
              " (4, 4),\n",
              " (5, 5),\n",
              " (6, 6),\n",
              " (7, 7),\n",
              " (8, 8),\n",
              " (9, 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}